% !TEX TS-program = xelatex

\documentclass[12pt]{report}
\usepackage[margin=1in]{geometry}

\usepackage{dkzhang}
\usepackage{amsmath}
\usepackage{hyperref}

\allowdisplaybreaks

\begin{document}



\chapter{Group Theory}

\begin{dfnbox}{Group}
	A \dfntxt{group} is an algebraic structure $\alg{G; 1, {}^{-1}, \cdot}$ consisting of:
	\begin{dfnitems}
		\item a set $G$, called the \dfntxt{underlying set};
		\item a distinguished element $1 \in G$, called the \dfntxt{identity element};
		\item a unary operation ${}^{-1}: G \to G$, written as $x \mapsto x^{-1}$, called \dfntxt{inversion};
		\item a binary operation $\cdot: G \times G \to G$, written as $(x, y) \mapsto x \cdot y$, called the \dfntxt{group operation} or \dfntxt{group product};
	\end{dfnitems}
	satisfying the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Associative property}: $(x \cdot y) \cdot z = x \cdot (y \cdot z)$ for all $x, y, z \in G$.
		\item \dfntxt{Identity property}: $1 \cdot x = x \cdot 1 = x$ for all $x \in G$.
		\item \dfntxt{Inverse property}: $x \cdot x^{-1} = x^{-1} \cdot x = 1$ for all $x \in G$.
	\end{dfnitems}
\end{dfnbox}

\begin{thmbox}{Cancellation Laws}
	\textbf{Theorem:} Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group, and let $x, y, z \in G$.
	\begin{dfnitems}
		\item \dfntxt{Left cancellation law}: If $x \cdot y = x \cdot z$, then $y = z$.
		\item \dfntxt{Right cancellation law}: If $x \cdot z = y \cdot z$, then $x = y$.
	\end{dfnitems}
\tcblower
	\textit{Proof:} If $x \cdot y = x \cdot z$, then:
	\begin{align*}
		y & = 1 \cdot y                &  & \text{(identity property)}    \\
		  & = (x^{-1} \cdot x) \cdot y &  & \text{(inverse property)}     \\
		  & = x^{-1} \cdot (x \cdot y) &  & \text{(associative property)} \\
		  & = x^{-1} \cdot (x \cdot z) &  & \text{(by hypothesis)}        \\
		  & = (x^{-1} \cdot x) \cdot z &  & \text{(associative property)} \\
		  & = 1 \cdot z                &  & \text{(inverse property)}     \\
		  & = z                        &  & \text{(identity property)}
	\end{align*}
	Similarly, if $x \cdot z = y \cdot z$, then
	\[ x = x \cdot 1 = x \cdot (z \cdot z^{-1}) = (x \cdot z) \cdot z^{-1} = (y \cdot z) \cdot z^{-1} = y \cdot (z \cdot z^{-1}) = y \cdot 1 = y. \]
\end{thmbox}

\begin{thmbox}{Uniqueness of Inverses} \label{grpuniq}
	\textbf{Theorem:} Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group, and let $x, y \in G$. If $x \cdot y = 1$ or $y \cdot x = 1$, then $y = x^{-1}$.
\tcblower
	\textit{Proof:} If $x \cdot y = 1$, then
	\[ y = 1 \cdot y = (x^{-1} \cdot x) \cdot y = x^{-1} \cdot (x \cdot y) = x^{-1} \cdot 1 = x^{-1}. \]
	Similarly, if $y \cdot x = 1$, then
	\[ y = y \cdot 1 = y \cdot (x \cdot x^{-1}) = (y \cdot x) \cdot x^{-1} = 1 \cdot x^{-1} = x^{-1}. \]
\end{thmbox}

\begin{thmbox}{Inversion is an Involution}
	\textbf{Theorem:} Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group. For all $x \in G$, we have $(x^{-1})^{-1} = x$.
\tcblower
	\textit{Proof:} By the uniqueness of inverses, in order to show that $x$ is the inverse of $x^{-1}$, it suffices to show that $x \cdot x^{-1} = 1$. This is guaranteed by the inverse property.
\end{thmbox}

\begin{thmbox}{Identity Element is its own Inverse}
	\textbf{Theorem:} If $\alg{G; 1, {}^{-1}, \cdot}$ is a group, then $1^{-1} = 1$.
\tcblower
	\textit{Proof:} By the uniqueness of inverses, in order to show that $1$ is its own inverse, it suffices to show that $1 \cdot 1 = 1$. This is guaranteed by the identity property.
\end{thmbox}

When dealing with more than one group in the same context, it is often helpful to label the identity elements, inversion operations, and product operations by the name of the group they belong to. For example, instead of naming the elements of a group $\alg{G; 1, {}^{-1}, \cdot}$, we might choose to name them $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$.

\begin{dfnbox}{Group Homomorphism}
	Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups. A \dfntxt{group homomorphism} is a function $f: G \to H$ that satisfies the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Preserves the identity}: $f(1_G) = 1_H$.
		\item \dfntxt{Preserves inverses}: $f(x^{-1}_G) = f(x)^{-1}_H$ for all $x \in G$.
		\item \dfntxt{Preserves products}: $f(x \cdot_G y) = f(x) \cdot_H f(y)$ for all $x, y \in G$.
	\end{dfnitems}
\end{dfnbox}

\begin{thmbox}{Preserving Products is Sufficient}
	\textbf{Theorem:} Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups. If a function $f: G \to H$ satisfies $f(x \cdot_G y) = f(x) \cdot_H f(y)$ for all $x, y \in G$, then $f$ is a group homomorphism.
\tcblower
	\textit{Proof:} We must show that $f$ preserves the identity and inverses. For preservation of the identity, we apply preservation of products to the equation $1_G = 1_G \cdot_G 1_G$ to conclude that
	\[ f(1_G) = f(1_G \cdot_G 1_G) = f(1_G) \cdot_H f(1_G). \]
	By cancellation, it follows that $f(1_G) = 1_H$. For preservation of inverses, let $x \in G$ be given. Since $1_G = x \cdot_G x^{-1}_G$, we can apply preservation of products and the identity to write
	\[ 1_H = f(1_G) = f(x \cdot_G x^{-1}_G) = f(x) \cdot_H f(x^{-1}_G). \]
	By uniqueness of inverses, it follows that $f(x^{-1}_G) = f(x)^{-1}_H$.
\end{thmbox}

\begin{dfnbox}{Kernel, $\ker f$}
	Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups, and let $f: G \to H$ be a group homomorphism. The \dfntxt{kernel} of $f$ is the subset $\ker f \subseteq G$ defined by
	\[ \ker f \coloneq \{ x \in G : f(x) = 1_H \}. \]
\end{dfnbox}

\begin{dfnbox}{Subgroup, $H \le G$}
	Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group. A \dfntxt{subgroup} of $\alg{G; 1, {}^{-1}, \cdot}$ is a subset $H \subseteq G$ such that $\alg{H; 1, {}^{-1}|_H, \cdot|_H}$ is a group, where ${}^{-1}|_H$ denotes the restriction of ${}^{-1}$ to $H \subseteq G$, and $\cdot|_H$ denotes the restriction of $\cdot$ to $H \times H \subseteq G \times G$. Explicitly, this means that:
	\begin{dfnitems}
		\item \dfntxt{Contains the identity}: $1 \in H$.
		\item \dfntxt{Closed under inverses}: If $x \in H$, then $x^{-1} \in H$.
		\item \dfntxt{Closed under products}: If $x, y \in H$, then $x \cdot y \in H$.
	\end{dfnitems}
	We write $H \le G$ to denote that $H$ is a subgroup of $\alg{G; 1, {}^{-1}, \cdot}$.
\end{dfnbox}

\begin{thmbox}{Kernels are Subgroups}
	\textbf{Theorem:} Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups. If $f: G \to H$ is a group homomorphism, then $\ker f$ is a subgroup of $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$.
\tcblower
	\textit{Proof:} We need to verify that $\ker f$ contains the identity, is closed under inverses, and is closed under products.
	\begin{dfnitems}
		\item A homomorphism must preserve the identity, i.e., $f(1_G) = 1_H$, so $1_G \in \ker f$.
		\item Let $x \in \ker f$ be given. By applying $f$ to both sides of the equation $1_G = x \cdot_G x^{-1}_G$, we obtain
		\[ 1_H = f(1_G) = f(x \cdot_G x^{-1}_G) = f(x) \cdot_H f(x^{-1}_G) = 1_H \cdot_H f(x^{-1}_G) = f(x^{-1}_G) \]
		which proves that $x^{-1}_G \in \ker f$.
		\item If $x, y \in \ker f$, then
		\[ f(x \cdot_G y) = f(x) \cdot_H f(y) = 1_H \cdot_H 1_H = 1_H \]
		which proves that $x \cdot_G y \in \ker f$.
	\end{dfnitems}
\end{thmbox}

\begin{itemize}
	\item We will henceforth refer to a group $\alg{G; 1, {}^{-1}, \cdot}$ simply by the name of its underlying set $G$, omitting explicit mention of its identity element, inversion operation, and product operation. Thus, instead of saying ``let $\alg{G; 1, {}^{-1}, \cdot}$ be a group,'' we will simply say ``let $G$ be a group.''
	\item When discussing a particular group $G$, the symbols $1$, ${}^{-1}$, and $\cdot$ will be implicitly understood to refer to identity element, inversion operation, and product operation of the group $G$ under discussion. When multiple groups are being discussed simultaneously, we will disambiguate these symbols using the name of the underlying set of the group they belong to (for example, $1_G$ and $\cdot_G$).
	\item Nested products of group elements will no longer be written with parentheses. The requirement of associativity guarantees that $x \cdot (y \cdot z) = (x \cdot y) \cdot z$, so we may write $x \cdot y \cdot z$ without fear of ambiguity. We will freely and implicitly apply the associative property whenever it is required to interpret a nested product in more than one way.
	\item Group operations will no longer be denoted by the symbol $\cdot$, but merely by juxtaposition. What we previously wrote as $x \cdot y$ will now simply be denoted by $xy$.
\end{itemize}

\begin{thmbox}{Inverse of a Product is the Reverse Product of Inverses}
	\textbf{Theorem:} Let $G$ be a group and $n \in \N$. If $x_1, \dots, x_n \in G$, then $(x_1 x_2 \cdots x_n)^{-1} = x_n^{-1} \cdots x_2^{-1} x_1^{-1}$.
\tcblower
	\textit{Proof:} By the uniqueness of inverses, it suffices to show that $x_1 x_2 \cdots x_n x_n^{-1} \cdots x_2^{-1} x_1^{-1} = 1$. We proceed by induction on $n$. The base case $n = 1$ holds by the inverse property $x_1 x_1^{-1} = 1$. Supposing that the claim holds for $n = k$, we establish the claim for $n = k + 1$ by calculating as follows:
	\[ x_1 x_2 \cdots x_k x_{k+1} x_{k+1}^{-1} x_k^{-1} \cdots x_2^{-1} x_1^{-1} = x_1 x_2 \cdots x_k x_k^{-1} \cdots x_2^{-1} x_1^{-1} = 1 \]
	The first equality follows from the inverse property $x_{k+1} x_{k+1}^{-1} = 1$, and the second equality follows from the inductive hypothesis.
\end{thmbox}

\begin{dfnbox}{Left Conjugate, Right Conjugate}
	Let $G$ be a group, and let $g, x \in G$. The \dfntxt{left conjugate} of $x$ by $g$ is the element ${}^g x \in G$ defined by
	\[ {}^g x \coloneq g x g^{-1}. \]
	Similarly, the \dfntxt{right conjugate} of $x$ by $g$ is the element $x^g \in G$ defined by
	\[ x^g \coloneq g^{-1} x g. \]
\end{dfnbox}

\begin{thmbox}{Properties of Conjugation}
	\textbf{Theorem:} Let $G$ be a group. For all $g, h, x \in G$, we have:
	\begin{dfnitems}
		\item ${}^1 x = x^1 = x$
		\item ${}^{g^{-1}} x = x^g$
		\item $x^{g^{-1}} = {}^g x$
		\item ${}^{g} ({}^{h} x) = {}^{gh} x$
		\item $(x^g)^h = x^{gh}$
	\end{dfnitems}
\tcblower
	\textit{Proof:} Let $g, h, x \in G$ be given.
	\[ {}^1 x = 1 x 1^{-1} = x1 = x = 1x = 1^{-1} x 1 = x^1 \]
	\[ {}^{g^{-1}} x = g^{-1} x (g^{-1})^{-1} = g^{-1} x g = x^g \]
	\[ x^{g^{-1}} = (g^{-1})^{-1} x g^{-1} = g x g^{-1} = {}^g x \]
	\[ {}^{g} ({}^{h} x) = g ({}^h x) g^{-1} = g h x h^{-1} g^{-1} = (gh) x (gh)^{-1} = {}^{gh} x \]
	\[ (x^g)^h = h^{-1} (x^g) h = h^{-1} g^{-1} x g x = (gh)^{-1} x (gh) = x^{gh} \]
\end{thmbox}

\begin{dfnbox}{Normal Subgroup, $N \normalin G$}
	Let $G$ be a group. A \dfntxt{normal subgroup} of $G$ is a subgroup $N \le G$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Closed under conjugation}: If $g \in G$ and $x \in N$, then $x^g \in N$.
	\end{dfnitems}
	We write $N \normalin G$ to denote that $N$ is a normal subgroup of $G$.
\end{dfnbox}

\begin{dfnbox}{Abelian Group}
	An \dfntxt{abelian group} is a group $G$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Commutative property}: $x y = y x$ for all $x, y \in G$.
	\end{dfnitems}
\end{dfnbox}

When a group is abelian, it is customary to adopt a different notational convention. Instead of the \dfntxt{multiplicative notation} $\alg{G; 1, {}^{-1}, \cdot}$, for abelian groups we use the \dfntxt{additive notation} $\alg{G; 0, -, +}$.

\begin{dfnbox}{Integer Powers of Group Elements, $x^n$}
	Let $\alg{G; 1_G, {}^{-1}, \cdot}$ be a group, $x \in G$, and $n \in \Z$. We denote by $x^n$ the element of $G$ defined as follows:
	\begin{dfnitems}
		\item If $n = 0$, then we define $x^0 \coloneq 1_G$.
		\item For $n > 0$, we define $x^n$ inductively as $x^n \coloneq x^{n-1} \cdot x$.
		\item For $n < 0$, we define $x^n$ inductively as $x^n \coloneq x^{n+1} \cdot x^{-1}$.
	\end{dfnitems}
\end{dfnbox}

For example, $x^2 \coloneq x \cdot x$ and $x^{-3} \coloneq x^{-1} \cdot x^{-1} \cdot x^{-1}$. This definition can create ambiguities with the notation $x^g$ for right conjugation if we do not carefully distinguish between group elements and integers. Thankfully, the potentially-problematic case $x^1$ causes no issues, as $x^1 = x$ regardless of whether we interpret $x^1$ as $x$ raised to the first power or the right conjugate of $x$ by $1$.

\begin{dfnbox}{Cyclic Group, Generator}
	A \dfntxt{cyclic group} is a group $G$ containing an element $x \in G$ such that every element $y \in G$ can be written as $y = x^n$ for some $n \in \Z$. Such an element $x$ is called the \dfntxt{generator} of the group $G$, and we write $G = \gen{x}$ to denote that $G$ is generated by $x$.
\end{dfnbox}



\chapter{Ring Theory}

In this chapter, we introduce a new class of algebraic structures, called rngs and rings, whose study is collectively called \dfntxt{ring theory}. Rngs and rings are more complicated than groups because their definition involves not one, but two binary operations.

\begin{dfnbox}{Rng}
	A \dfntxt{rng} (pronounced as ``\textit{rung}'') is an algebraic structure $\alg{R; 0, -, +, \cdot}$ consisting of:
	\begin{dfnitems}
		\item a set $R$, called the \dfntxt{underlying set};
		\item a distinguished element $0 \in R$, called the \dfntxt{zero element};
		\item a unary operation $-: R \to R$, written as $x \mapsto -x$, called \dfntxt{negation};
		\item a binary operation $+: R \times R \to R$, written as $(x, y) \mapsto x + y$, called \dfntxt{addition};
		\item a binary operation $\cdot: R \times R \to R$, written as $(x, y) \mapsto x \cdot y$, called \dfntxt{multiplication};
	\end{dfnitems}
	satisfying the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Additive structure}: $\alg{R; 0, -, +}$ is an abelian group.
		\item \dfntxt{Associativity}: $(x \cdot y) \cdot z = x \cdot (y \cdot z)$ for all $x, y, z \in R$.
		\item \dfntxt{Left distributivity}: $x \cdot (y + z) = (x \cdot y) + (x \cdot z)$ for all $x, y, z \in R$.
		\item \dfntxt{Right distributivity}: $(x + y) \cdot z = (x \cdot z) + (y \cdot z)$ for all $x, y, z \in R$.
	\end{dfnitems}
\end{dfnbox}

The key ingredient present in the definition of a rng is \dfntxt{distributivity}, which establishes a link between two different binary operations. We begin our study of rngs by proving a simple (but important!) result to demonstrate the utility of the distributive property.

\begin{thmbox}{Multiplying by Zero Yields Zero}
	\textbf{Theorem:} Let $\alg{R; 0, -, +, \cdot}$ be a rng. For any $x \in R$, we have $0 \cdot x = x \cdot 0 = 0$.
\tcblower
	\textit{Proof:} Let $x \in R$ be given. Because $0$ is the identity element of the abelian group $\langle R; 0, -, + \rangle$, we have $0 = 0 + 0$. Using left distributivity, it follows that $0 \cdot x = (0 + 0) \cdot x = (0 \cdot x) + (0 \cdot x)$, and by canceling one copy of $0 \cdot x$ on both sides, we conclude that $0 \cdot x = 0$. We similarly apply right distributivity to the expression $x \cdot 0 = x \cdot (0 + 0) = (x \cdot 0) + (x \cdot 0)$ to conclude that $x \cdot 0 = 0$.
\end{thmbox}

\begin{dfnbox}{Subrng, $S \le R$}
\end{dfnbox}

\begin{dfnbox}{Rng Homomorphism}
	Let $\alg{R; 0_R, -_R +_R, \cdot_R}$ and $\alg{S; 0_S, -_S +_S, \cdot_S}$ be rngs. A \dfntxt{rng homomorphism} is a function $f: R \to S$ that satisfies the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Preserves additive structure}: $f$ is a group homomorphism between the abelian groups $\alg{R; 0_R, -_R +_R}$ and $\alg{S; 0_S, -_S +_S}$.
		\item \dfntxt{Preserves products}: $f(x \cdot_R y) = f(x) \cdot_S f(y)$ for all $x, y \in R$.
	\end{dfnitems}
\end{dfnbox}

As with groups, we will often refer to a rng $\alg{R; 0, -, +, \cdot}$ by the name of its underlying set $R$ and denote the multiplication operation $\cdot$ by juxtaposition. We will never denote addition, subtraction, or negation by juxtaposition, so the symbols $+$ and $-$ will always be used.

\begin{dfnbox}{Ideal, Left Ideal, Right Ideal, One-Sided Ideal, Two-Sided Ideal, $I \normalin R$}
	Let $R$ be a rng.
	\begin{dfnitems}
		\item A \dfntxt{left ideal} of $R$ is a subrng $I \le R$ that satisfies the following additional requirement:
		\begin{dfnitems}
			\item \dfntxt{Absorbs left multiplication}: $rx \in I$ for all $r \in R$ and $x \in I$.
		\end{dfnitems}
		\item A \dfntxt{right ideal} of $R$ is a subrng $I \le R$ that satisfies the following additional requirement:
		\begin{dfnitems}
			\item \dfntxt{Absorbs right multiplication}: $xr \in I$ for all $x \in I$ and $r \in R$.
		\end{dfnitems}
		\item A \dfntxt{one-sided ideal} of $R$ is a subrng $I \le R$ that is a left ideal or a right ideal (possibly both).
		\item A \dfntxt{two-sided ideal} of $R$, or simply an \dfntxt{ideal} of $R$, is a subrng $I \le R$ that is both a left ideal and a right ideal.
	\end{dfnitems}
	We write $I \normalin R$ to denote that $I$ is a (two-sided) ideal of $R$.
\end{dfnbox}

Every two-sided ideal is also a one-sided ideal, but the converse is not true.

\begin{thmbox}{Kernels are Ideals}
	\textbf{Theorem:} If $f: R \to S$ is a rng homomorphism, then $\ker f \normalin R$.
\end{thmbox}

\begin{dfnbox}{Ring}
	A \dfntxt{ring} is an algebraic structure $\alg{R; 0, 1, -, +, \cdot}$ consisting of a rng $\alg{R; 0, -, +, \cdot}$ and a distinguished element $1 \in R$, called the \dfntxt{identity element}, satisfying the following requirement:
	\begin{dfnitems}
		\item \dfntxt{Identity}: $1 \cdot x = x \cdot 1 = x$ for all $x \in R$.
	\end{dfnitems}
\end{dfnbox}

Note that the distinguished elements $0$ and $1$ in the definition of a ring need not be distinct. There is one ring in which $0 = 1$ holds.

\begin{thmbox}{$0=1$ Implies that a Ring is Trivial}
	\textbf{Theorem:} Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. If $0 = 1$, then $R = \{0\}$.
\tcblower
	\textit{Proof:} For any $x \in R$, we have $x = 1 \cdot x = 0 \cdot x = 0$.
\end{thmbox}

\begin{dfnbox}{Inverse, Left Inverse, Right Inverse, Two-Sided Inverse, Invertible, Unit}
	Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring, and let $x \in R$.
	\begin{dfnitems}
		\item A \dfntxt{left inverse} of $x$ is an element $y \in R$ such that $y \cdot x = 1$. If such an element exists, then we say that $x$ is \dfntxt{left-invertible}.
		\item A \dfntxt{right inverse} of $x$ is an element $y \in R$ such that $x \cdot y = 1$. If such an element exists, then we say that $x$ is \dfntxt{right-invertible}.
		\item A \dfntxt{two-sided inverse} of $x$, or simply an \dfntxt{inverse} of $x$, is an element $y \in R$ such that $y \cdot x = x \cdot y = 1$. If such an element exists, then we say that $x$ is \dfntxt{invertible}, and we call $x$ a \dfntxt{unit}.
	\end{dfnitems}
\end{dfnbox}

In ring theory, the word ``inverse'' used without further elaboration usually means ``two-sided inverse.''

\begin{thmbox}{Left and Right Invertibility Imply Two-Sided Invertibility}
	\textbf{Theorem:} Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. If $x \in R$ has both a left inverse $y \in R$ and a right inverse $z \in R$, then $y = z$, and $x$ is invertible.
\tcblower
	\textit{Proof:} Using the associativity of multiplication, observe that
	\[ y = y \cdot 1 = y \cdot (x \cdot z) = (y \cdot x) \cdot z = 1 \cdot z = z. \]
	Hence, $y = z$ is a two-sided inverse of $x$.
\end{thmbox}

\begin{thmbox}{Inverses are Unique and Invertible}
	\textbf{Corollary:} Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. If an element $x \in R$ is invertible, then it has a unique inverse. Moreover, that inverse is itself invertible, and $x$ is its unique inverse.
\tcblower
	\textit{Proof:} Let $y,z \in R$ be (two-sided) inverses of $x$. In particular, $y$ is a left inverse of $x$, and $z$ is a right inverse of $x$, so we can apply the preceding result to conclude that $y = z$.

	Observe that the relation $x \cdot y = y \cdot x = 1$ that defines inverses is symmetric in $x$ and $y$. Hence, if $y$ is an inverse of $x$, then $x$ is also an inverse of $y$.
\end{thmbox}

This result allows us to speak unambiguously of \textit{the} inverse of an invertible element of a ring.

\begin{dfnbox}{$R^\times$, $x^{-1}$}
	Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. The set of all units in $R$ is denoted by $R^\times$. For each $x \in R^\times$, we denote by $x^{-1}$ the unique inverse of $x$. Thus, we regard the map $x \mapsto x^{-1}$ as a unary operation ${}^{-1}: R^\times \to R^\times$.
\end{dfnbox}

Using this notation, we can restate the preceding result as $x = (x^{-1})^{-1}$ for all $x \in R^\times$.

\begin{thmbox}{$R^\times$ is a Group}
	\textbf{Theorem:} If $\alg{R; 0, 1, -, +, \cdot}$ is a ring, then $\alg{R^\times; 1, {}^{-1}, \cdot}$ is a group.
\end{thmbox}

\begin{thmbox}{An Ideal that Contains a Unit is the Entire Ring}
	\textbf{Theorem:} Let $R$ be a ring. If $I \le R$ is a one-sided ideal that contains a unit, then $I = R$.
\end{thmbox}

\begin{dfnbox}{Commutative Ring}
	A \dfntxt{commutative ring} is a ring $\alg{R; 0, 1, -, +, \cdot}$ that satisfies the following requirement:
	\begin{dfnitems}
		\item \dfntxt{Commutativity}: $x \cdot y = y \cdot x$ for all $x, y \in R$.
	\end{dfnitems}
\end{dfnbox}



\chapter{Field Theory}

\begin{dfnbox}{Field}
	A \dfntxt{field} is a commutative ring $\alg{K; 0, 1, -, +, \cdot}$ that satisfies the following additional requirements:
	\begin{dfnitems}
		\item \dfntxt{Nontriviality}: $0 \ne 1$.
		\item \dfntxt{Invertibility}: Every element of $K \setminus \{0\}$ has a (two-sided) inverse, i.e., $K^\times = K \setminus \{0\}$.
	\end{dfnitems}
\end{dfnbox}

\begin{dfnbox}{Field Homomorphism}
	A \dfntxt{field homomorphism} is a ring homomorphism $f: K \to L$ where the domain $K$ and codomain $L$ are both fields.
\end{dfnbox}

Note that field homomorphisms are defined as \textit{ring} homomorphisms, not \textit{rng} homomorphisms, so they are required to map $1$ to $1$.

We now prove a crucial fact that markedly distinguishes field theory from group theory and ring theory.

\begin{thmbox}{Fields Have Two Ideals}
	\textbf{Theorem:} Let $K$ be a field. If $I \le K$ is a one-sided ideal, then either $I = \{0\}$ or $I = K$.
\tcblower
	\textit{Proof:} A one-sided ideal $I \le K$ must contain $0$ by definition. If $I$ contains any other element of $K$, then $I$ contains a unit, and hence contains every element of $K$.
\end{thmbox}

\begin{thmbox}{Every Field Homomorphism is a Monomorphism}
	\textbf{Theorem:} Every rng homomorphism $f: K \to R$ from a field $K$ to a rng $R$ is either injective or trivial (i.e., $f(x) = 0$ for all $x \in K$).
\tcblower
	\textit{Proof:} Either $\ker f = \{0\}$, in which case $f$ is injective, or $\ker f = K$, in which case $f$ is trivial.
\end{thmbox}

In field theory, it is conventional to regard a monomorphism $f: K \injto L$ as an \textit{embedding} of $K$ into $L$. Under this interpretation, the preceding result shows that the only possible relationship between two fields (via a field homomorphism) is for one to be contained inside the other. For this reason, field theory does not adopt the language of homomorphisms that permeates group theory and ring theory. Instead, field theory is written in the language of \textit{subfields} and \textit{field extensions}.

\begin{dfnbox}{Subfield}
	Let $K$ be a field. A \dfntxt{subfield} of $K$ is a subring $L \le K$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Closed under inversion}: If $x \in L \setminus \{0\}$, then $x^{-1} \in L$.
	\end{dfnitems}
\end{dfnbox}

Not every subring of a field is a subfield. For example, $\Z$ is a subring but not a subfield of $\Q$.

\begin{dfnbox}{Field Extension, $L/K$}
	Let $L$ be a field. If $K$ is a subfield of $L$, then we say that $L$ is an \dfntxt{extension} of $K$, and we say that $L/K$ (pronounced as ``$L$ \textit{over} $K$'') is a \dfntxt{field extension}.
\end{dfnbox}

Somewhat confusingly, the notation $L/K$ is not intended to represent a quotient of any kind. It is simply a strange (but historically traditional) notation for an ordered pair $(L, K)$ of fields, carrying the additional information that the second field is contained in the first. Field theory has no use for quotients, since fields have no nontrivial proper ideals.

\begin{dfnbox}{Degree, $[L : K]$, Finite Extension}
	The \dfntxt{degree} of a field extension $L/K$, denoted by $[L : K]$, is the dimension of $L$ as a vector space over $K$. If $[L : K]$ is finite, then we call $L/K$ a \dfntxt{finite extension}.
\end{dfnbox}

Note that the individual fields $K$ and $L$ involved in a finite extension $L/K$ are allowed to have infinite cardinality. The phrase ``finite extension'' specifies that the \textit{extension} is finite, not that the \textit{fields} are finite.

\begin{dfnbox}{Algebraic Element, Transcendental Element}
	Let $L/K$ be a field extension. We say that an element $\alpha \in L$ is \dfntxt{algebraic} over $K$ if there exists a polynomial $p \in K[x]$ such that $p(\alpha) = 0$. If no such polynomial exists, then we say that $\alpha$ is \dfntxt{transcendental} over $K$.
\end{dfnbox}

\begin{dfnbox}{Algebraic Extension, Transcendental Extension}
	An \dfntxt{algebraic extension} is a field extension $L/K$ in which every element of $L$ is algebraic over $K$. On the other hand, a \dfntxt{transcendental extension} is a field extension $L/K$ in which $L$ contains an element that is transcendental over $K$.
\end{dfnbox}

\begin{thmbox}{Minimal Polynomials Exist}
	\textbf{Theorem:} Let $L/K$ be a field extension. If $\alpha \in L$ is algebraic over $K$, then there exists a unique monic polynomial in $K[x]$ of minimal degree satisfying $p(\alpha) = 0$.
\tcblower
	\textit{Proof:} The existence of such a polynomial is clear, since by the hypothesis that $\alpha$ is algebraic over $K$, there exists a polynomial $p \in K[x]$ satisfying $p(\alpha) = 0$. (This polynomial can be made monic by dividing it by its leading coefficient.) Because the degrees of monic polynomials (i.e., non-negative integers) are well-ordered, we can conclude that there exists a minimal-degree monic polynomial having this property.

	We verify uniqueness by contradiction. Suppose that there exist two distinct monic polynomials $p, q \in K[x]$ of minimal degree that satisfy $p(\alpha) = q(\alpha) = 0$. It follows that $p-q \in K[x]$ is a polynomial of strictly smaller degree that satisfies $(p-q)(\alpha) = p(\alpha) - q(\alpha) = 0 - 0 = 0$, contradicting the minimality of $p$ and $q$.
\end{thmbox}

\begin{dfnbox}{Minimal Polynomial}
	Let $L/K$ be a field extension, and let $\alpha \in L$ be algebraic over $K$. The \dfntxt{minimal polynomial} of $\alpha$ over $K$ is the unique monic polynomial $p \in K[x]$ of minimal degree satisfying $p(\alpha) = 0$.
\end{dfnbox}

\begin{thmbox}{Minimal Polynomials are Irreducible}
	\textbf{Theorem:} Let $L/K$ be a field extension. If $p \in K[x]$ is the minimal polynomial of an algebraic element $\alpha \in L$, then $p$ is irreducible in $K[x]$.
\tcblower
	\textit{Proof:} If $p$ could be written as the product of two non-constant polynomials $q, r \in K[x]$, then $0 = p(\alpha) = q(\alpha) r(\alpha)$ would imply that $q(\alpha) = 0$ or $r(\alpha) = 0$, contradicting the minimality of $p$.
\end{thmbox}

\begin{dfnbox}{Finite Field}
	A \dfntxt{finite field} is a field whose underlying set has finite cardinality.
\end{dfnbox}

\begin{thmbox}{Finite Fields have Cyclic Multiplicative Groups}
	\textbf{Theorem:} If $K$ is a finite field, then $K^\times$ is a cyclic group.
\end{thmbox}

\begin{dfnbox}{Simple Extension, Primitive Element}
	A field extension $L/K$ is called a \dfntxt{simple extension} if there exists an element $\alpha \in L$ such that $L = K(\alpha)$, i.e., every element of $L$ can be expressed as a rational function of $\alpha$ with coefficients in $K$. Such an element $\alpha$ is called a \dfntxt{primitive element} of $L$ over $K$.
\end{dfnbox}

\begin{dfnbox}
	Let $L/K$ be a field extension.
\end{dfnbox}

\begin{thmbox}{Primitive Element Theorem}
	\textbf{Theorem:} Let $K$ be a field. A finite extension $L/K$ is simple if and only if there exist finitely many intermediate subfields $F$ satisfying $K \le F \le L$.
\tcblower
	\textit{Proof:} We first consider the case of a finite base field $K$. In this case, the extension field $L$, being a finite extension of a finite field, is also finite. This implies that both sides of the desired ``if and only if'' statement are true. In particular, $L$ has finitely many subfields, and $L^\times$ is a cyclic group, so its generator is a primitive element.

	Now suppose that the base field $K$ is infinite. There are two implications that need to be proven.
	\begin{dfnitems}
		\item Suppose $L/K$ is a simple extension with primitive element $\alpha \in L$,
		\item Suppose there are finitely many intermediate subfields $F$ satisfying $K \le F \le L$.
	\end{dfnitems}
\end{thmbox}



\chapter{Differential Algebra}

\begin{dfnbox}{Derivation}
	A \dfntxt{derivation} on a rng $\alg{R; 0, -, +, \cdot}$ is a unary operation $\dd: R \to R$ that satisfies the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Additivity}: $\dd(x + y) = \dd(x) + \dd(y)$ for all $x, y \in R$.
		\item \dfntxt{Leibniz rule}: $\dd(x \cdot y) = \dd(x) \cdot y + x \cdot \dd(y)$ for all $x, y \in R$.
	\end{dfnitems}
\end{dfnbox}

\begin{dfnbox}{Differential Rng}
	A \dfntxt{differential rng} is an algebraic structure $\alg{R; 0, -, \dd, +, \cdot}$ consisting of a rng $\alg{R; 0, -, +, \cdot}$ and a derivation $\dd: R \to R$ on $\alg{R; 0, -, +, \cdot}$.
\end{dfnbox}

\begin{dfnbox}{Antiderivative}
	Let $R$ be a differential rng. An element $F \in R$ is an \dfntxt{antiderivative} of another element $f \in R$ if $\dd(F) = f$.
\end{dfnbox}

\begin{thmbox}{Algebraic Extensions Don't Create Antiderivatives}
	\textbf{Theorem:} Let $K$ be a differential field of characteristic zero. If $f \in K$ does not have an antiderivative in $K$, then $f$ cannot have an antiderivative in any algebraic differential extension of $K$.
\tcblower
	\textit{Proof:} We proceed by contraposition. Suppose that there exists an algebraic differential extension $L/K$ having an element $g \in L$ for which $f = \dd(g)$. Let $\Tr: K[g] \to K$ denote the trace map, and recall that $\Tr$ commutes with $\dd$. Since $f \in K$, we have $\Tr(f) = nf$, where $n \coloneq [K[g] : K]$. It follows that
	\[ \dd \left( \frac{\Tr(g)}{n} \right) = \frac{1}{n} \dd(\Tr(g)) = \frac{1}{n} \Tr(\dd(g)) = \frac{1}{n} \Tr(f) = f \]
	which shows that $\Tr(g)/n \in K$ is an antiderivative of $f$.
\end{thmbox}



\end{document}
