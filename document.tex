% !TeX TS-program = xelatex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US
% !TeX root = document.tex

\documentclass[12pt]{report}
\usepackage[margin=1in]{geometry}

\usepackage{dkzhang}
\usepackage{amsmath}
\usepackage{hyperref}

\allowdisplaybreaks

\begin{document}



\chapter{Group Theory}

\begin{dfnbox}{Group}
	A \dfntxt{group} is an algebraic structure $\alg{G; 1, {}^{-1}, \cdot}$ consisting of:
	\begin{dfnitems}
		\item a set $G$, called the \dfntxt{underlying set};
		\item a distinguished element $1 \in G$, called the \dfntxt{identity element};
		\item a unary operation ${}^{-1}: G \to G$, written as $x \mapsto x^{-1}$, called \dfntxt{inversion};
		\item a binary operation $\cdot: G \times G \to G$, written as $(x, y) \mapsto x \cdot y$, called the \dfntxt{group operation} or \dfntxt{group product};
	\end{dfnitems}
	satisfying the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Associative property}: $(x \cdot y) \cdot z = x \cdot (y \cdot z)$ for all $x, y, z \in G$.
		\item \dfntxt{Identity property}: $1 \cdot x = x \cdot 1 = x$ for all $x \in G$.
		\item \dfntxt{Inverse property}: $x \cdot x^{-1} = x^{-1} \cdot x = 1$ for all $x \in G$.
	\end{dfnitems}
\end{dfnbox}

\begin{thmbox}{Cancellation Laws}
	\textbf{Theorem:} Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group, and let $x, y, z \in G$.
	\begin{dfnitems}
		\item \dfntxt{Left cancellation law}: If $x \cdot y = x \cdot z$, then $y = z$.
		\item \dfntxt{Right cancellation law}: If $x \cdot z = y \cdot z$, then $x = y$.
	\end{dfnitems}
\tcblower
	\textit{Proof:} If $x \cdot y = x \cdot z$, then:
	\begin{align*}
		y & = 1 \cdot y                &  & \text{(identity property)}    \\
		  & = (x^{-1} \cdot x) \cdot y &  & \text{(inverse property)}     \\
		  & = x^{-1} \cdot (x \cdot y) &  & \text{(associative property)} \\
		  & = x^{-1} \cdot (x \cdot z) &  & \text{(by hypothesis)}        \\
		  & = (x^{-1} \cdot x) \cdot z &  & \text{(associative property)} \\
		  & = 1 \cdot z                &  & \text{(inverse property)}     \\
		  & = z                        &  & \text{(identity property)}
	\end{align*}
	Similarly, if $x \cdot z = y \cdot z$, then
	\[ x = x \cdot 1 = x \cdot (z \cdot z^{-1}) = (x \cdot z) \cdot z^{-1} = (y \cdot z) \cdot z^{-1} = y \cdot (z \cdot z^{-1}) = y \cdot 1 = y. \]
\end{thmbox}

\begin{thmbox}{Uniqueness of Inverses} \label{grpuniq}
	\textbf{Theorem:} Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group, and let $x, y \in G$. If $x \cdot y = 1$ or $y \cdot x = 1$, then $y = x^{-1}$.
\tcblower
	\textit{Proof:} If $x \cdot y = 1$, then
	\[ y = 1 \cdot y = (x^{-1} \cdot x) \cdot y = x^{-1} \cdot (x \cdot y) = x^{-1} \cdot 1 = x^{-1}. \]
	Similarly, if $y \cdot x = 1$, then
	\[ y = y \cdot 1 = y \cdot (x \cdot x^{-1}) = (y \cdot x) \cdot x^{-1} = 1 \cdot x^{-1} = x^{-1}. \]
\end{thmbox}

\begin{thmbox}{Inversion is an Involution}
	\textbf{Theorem:} Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group. For all $x \in G$, we have $(x^{-1})^{-1} = x$.
\tcblower
	\textit{Proof:} By the uniqueness of inverses, in order to show that $x$ is the inverse of $x^{-1}$, it suffices to show that $x \cdot x^{-1} = 1$. This is guaranteed by the inverse property.
\end{thmbox}

\begin{thmbox}{Identity Element is its own Inverse}
	\textbf{Theorem:} If $\alg{G; 1, {}^{-1}, \cdot}$ is a group, then $1^{-1} = 1$.
\tcblower
	\textit{Proof:} By the uniqueness of inverses, in order to show that $1$ is its own inverse, it suffices to show that $1 \cdot 1 = 1$. This is guaranteed by the identity property.
\end{thmbox}

When dealing with more than one group in the same context, it is often helpful to label the identity elements, inversion operations, and product operations by the name of the group they belong to. For example, instead of naming the elements of a group $\alg{G; 1, {}^{-1}, \cdot}$, we might choose to name them $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$.

\begin{dfnbox}{Group Homomorphism}
	Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups. A \dfntxt{group homomorphism} is a function $f: G \to H$ that satisfies the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Preserves the identity}: $f(1_G) = 1_H$.
		\item \dfntxt{Preserves inverses}: $f(x^{-1}_G) = f(x)^{-1}_H$ for all $x \in G$.
		\item \dfntxt{Preserves products}: $f(x \cdot_G y) = f(x) \cdot_H f(y)$ for all $x, y \in G$.
	\end{dfnitems}
\end{dfnbox}

\begin{thmbox}{Preserving Products is Sufficient}
	\textbf{Theorem:} Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups. If a function $f: G \to H$ satisfies $f(x \cdot_G y) = f(x) \cdot_H f(y)$ for all $x, y \in G$, then $f$ is a group homomorphism.
\tcblower
	\textit{Proof:} We must show that $f$ preserves the identity and inverses. For preservation of the identity, we apply preservation of products to the equation $1_G = 1_G \cdot_G 1_G$ to conclude that
	\[ f(1_G) = f(1_G \cdot_G 1_G) = f(1_G) \cdot_H f(1_G). \]
	By cancellation, it follows that $f(1_G) = 1_H$. For preservation of inverses, let $x \in G$ be given. Since $1_G = x \cdot_G x^{-1}_G$, we can apply preservation of products and the identity to write
	\[ 1_H = f(1_G) = f(x \cdot_G x^{-1}_G) = f(x) \cdot_H f(x^{-1}_G). \]
	By uniqueness of inverses, it follows that $f(x^{-1}_G) = f(x)^{-1}_H$.
\end{thmbox}

\begin{dfnbox}{Kernel, $\ker f$}
	Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups, and let $f: G \to H$ be a group homomorphism. The \dfntxt{kernel} of $f$ is the subset $\ker f \subseteq G$ defined by
	\[ \ker f \coloneq \{ x \in G : f(x) = 1_H \}. \]
\end{dfnbox}

\begin{dfnbox}{Subgroup, $H \le G$}
	Let $\alg{G; 1, {}^{-1}, \cdot}$ be a group. A \dfntxt{subgroup} of $\alg{G; 1, {}^{-1}, \cdot}$ is a subset $H \subseteq G$ such that $\alg{H; 1, {}^{-1}|_H, \cdot|_H}$ is a group, where ${}^{-1}|_H$ denotes the restriction of ${}^{-1}$ to $H \subseteq G$, and $\cdot|_H$ denotes the restriction of $\cdot$ to $H \times H \subseteq G \times G$. Explicitly, this means that:
	\begin{dfnitems}
		\item \dfntxt{Contains the identity}: $1 \in H$.
		\item \dfntxt{Closed under inverses}: If $x \in H$, then $x^{-1} \in H$.
		\item \dfntxt{Closed under products}: If $x, y \in H$, then $x \cdot y \in H$.
	\end{dfnitems}
	We write $H \le \alg{G; 1, {}^{-1}, \cdot}$ to denote that $H$ is a subgroup of $\alg{G; 1, {}^{-1}, \cdot}$.
\end{dfnbox}

\begin{thmbox}{Kernels are Subgroups}
	\textbf{Theorem:} Let $\alg{G; 1_G, {}^{-1}_G, \cdot_G}$ and $\alg{H; 1_H, {}^{-1}_H, \cdot_H}$ be groups. If $f: G \to H$ is a group homomorphism, then $\ker f \le \alg{G; 1_G, {}^{-1}_G, \cdot_G}$.
\tcblower
	\textit{Proof:} We need to verify that $\ker f$ contains the identity, is closed under inverses, and is closed under products.
	\begin{dfnitems}
		\item A homomorphism must preserve the identity, i.e., $f(1_G) = 1_H$, so $1_G \in \ker f$.
		\item Let $x \in \ker f$ be given. By applying $f$ to both sides of the equation $1_G = x \cdot_G x^{-1}_G$, we obtain
		\[ 1_H = f(1_G) = f(x \cdot_G x^{-1}_G) = f(x) \cdot_H f(x^{-1}_G) = 1_H \cdot_H f(x^{-1}_G) = f(x^{-1}_G) \]
		which proves that $x^{-1}_G \in \ker f$.
		\item If $x, y \in \ker f$, then
		\[ f(x \cdot_G y) = f(x) \cdot_H f(y) = 1_H \cdot_H 1_H = 1_H \]
		which proves that $x \cdot_G y \in \ker f$.
	\end{dfnitems}
\end{thmbox}

\begin{itemize}
	\item We will henceforth refer to a group $\alg{G; 1, {}^{-1}, \cdot}$ simply by the name of its underlying set $G$, omitting explicit mention of its identity element, inversion operation, and product operation. Thus, instead of saying ``let $\alg{G; 1, {}^{-1}, \cdot}$ be a group,'' we will simply say ``let $G$ be a group.''
	\item When discussing a particular group $G$, the symbols $1$, ${}^{-1}$, and $\cdot$ will be implicitly understood to refer to identity element, inversion operation, and product operation of the group $G$ under discussion. When multiple groups are being discussed simultaneously, we will disambiguate these symbols using the name of the underlying set of the group they belong to (for example, $1_G$ and $\cdot_G$).
	\item Nested products of group elements will no longer be written with parentheses. The requirement of associativity guarantees that $x \cdot (y \cdot z) = (x \cdot y) \cdot z$, so we may write $x \cdot y \cdot z$ without fear of ambiguity. We will freely and implicitly apply the associative property whenever it is required to interpret a nested product in more than one way.
	\item Group operations will no longer be denoted by the symbol $\cdot$, but merely by juxtaposition. What we previously wrote as $x \cdot y$ will now simply be denoted by $xy$.
\end{itemize}

\begin{thmbox}{Inverse of a Product is the Reverse Product of Inverses}
	\textbf{Theorem:} Let $G$ be a group and $n \in \N$. If $x_1, \dots, x_n \in G$, then $(x_1 x_2 \cdots x_n)^{-1} = x_n^{-1} \cdots x_2^{-1} x_1^{-1}$.
\tcblower
	\textit{Proof:} By the uniqueness of inverses, it suffices to show that $x_1 x_2 \cdots x_n x_n^{-1} \cdots x_2^{-1} x_1^{-1} = 1$. We proceed by induction on $n$. The base case $n = 1$ holds by the inverse property $x_1 x_1^{-1} = 1$. Supposing that the claim holds for $n = k$, we establish the claim for $n = k + 1$ by calculating as follows:
	\[ x_1 x_2 \cdots x_k x_{k+1} x_{k+1}^{-1} x_k^{-1} \cdots x_2^{-1} x_1^{-1} = x_1 x_2 \cdots x_k x_k^{-1} \cdots x_2^{-1} x_1^{-1} = 1 \]
	The first equality follows from the inverse property $x_{k+1} x_{k+1}^{-1} = 1$, and the second equality follows from the inductive hypothesis.
\end{thmbox}

\begin{dfnbox}{Left Conjugate, Right Conjugate}
	Let $G$ be a group, and let $g, x \in G$. The \dfntxt{left conjugate} of $x$ by $g$ is the element ${}^g x \in G$ defined by
	\[ {}^g x \coloneq g x g^{-1}. \]
	Similarly, the \dfntxt{right conjugate} of $x$ by $g$ is the element $x^g \in G$ defined by
	\[ x^g \coloneq g^{-1} x g. \]
\end{dfnbox}

\begin{thmbox}{Properties of Conjugation}
	\textbf{Theorem:} Let $G$ be a group. For all $g, h, x \in G$, we have:
	\begin{dfnitems}
		\item ${}^1 x = x^1 = x$
		\item ${}^{g^{-1}} x = x^g$
		\item $x^{g^{-1}} = {}^g x$
		\item ${}^{g} ({}^{h} x) = {}^{gh} x$
		\item $(x^g)^h = x^{gh}$
	\end{dfnitems}
\tcblower
	\textit{Proof:} Let $g, h, x \in G$ be given.
	\[ {}^1 x = 1 x 1^{-1} = x1 = x = 1x = 1^{-1} x 1 = x^1 \]
	\[ {}^{g^{-1}} x = g^{-1} x (g^{-1})^{-1} = g^{-1} x g = x^g \]
	\[ x^{g^{-1}} = (g^{-1})^{-1} x g^{-1} = g x g^{-1} = {}^g x \]
	\[ {}^{g} ({}^{h} x) = g ({}^h x) g^{-1} = g h x h^{-1} g^{-1} = (gh) x (gh)^{-1} = {}^{gh} x \]
	\[ (x^g)^h = h^{-1} (x^g) h = h^{-1} g^{-1} x g x = (gh)^{-1} x (gh) = x^{gh} \]
\end{thmbox}

\begin{dfnbox}{Normal Subgroup, $N \normalin G$}
	Let $G$ be a group. A \dfntxt{normal subgroup} of $G$ is a subgroup $N \le G$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Closed under conjugation}: If $g \in G$ and $x \in N$, then $x^g \in N$.
	\end{dfnitems}
	We write $N \normalin G$ to denote that $N$ is a normal subgroup of $G$.
\end{dfnbox}

\begin{dfnbox}{Abelian Group}
	An \dfntxt{abelian group} is a group $G$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Commutative property}: $x y = y x$ for all $x, y \in G$.
	\end{dfnitems}
\end{dfnbox}

When a group is abelian, it is customary to adopt a different notational convention. Instead of the \dfntxt{multiplicative notation} $\alg{G; 1, {}^{-1}, \cdot}$, for abelian groups we use the \dfntxt{additive notation} $\alg{G; 0, -, +}$.

\begin{dfnbox}{Integer Powers of Group Elements, $x^n$}
	Let $\alg{G; 1_G, {}^{-1}, \cdot}$ be a group, $x \in G$, and $n \in \Z$. We denote by $x^n$ the element of $G$ defined as follows:
	\begin{dfnitems}
		\item If $n = 0$, then we define $x^0 \coloneq 1_G$.
		\item For $n > 0$, we define $x^n$ inductively as $x^n \coloneq x^{n-1} \cdot x$.
		\item For $n < 0$, we define $x^n$ inductively as $x^n \coloneq x^{n+1} \cdot x^{-1}$.
	\end{dfnitems}
\end{dfnbox}

For example, $x^2 \coloneq x \cdot x$ and $x^{-3} \coloneq x^{-1} \cdot x^{-1} \cdot x^{-1}$. This definition can create ambiguities with the notation $x^g$ for right conjugation if we do not carefully distinguish between group elements and integers. Thankfully, the potentially-problematic case $x^1$ causes no issues, as $x^1 = x$ regardless of whether we interpret $x^1$ as $x$ raised to the first power or the right conjugate of $x$ by $1$.

\begin{dfnbox}{Cyclic Group, Generator}
	A \dfntxt{cyclic group} is a group $G$ containing an element $x \in G$ such that every element $y \in G$ can be written as $y = x^n$ for some $n \in \Z$. Such an element $x$ is called the \dfntxt{generator} of the group $G$, and we write $G = \gen{x}$ to denote that $G$ is generated by $x$.
\end{dfnbox}



\chapter{Ring Theory}

In this chapter, we introduce a new class of algebraic structures, called rngs and rings, whose study is collectively called \dfntxt{ring theory}. Rngs and rings are more complicated than groups because their definition involves not one, but two binary operations.

\begin{dfnbox}{Rng}
	A \dfntxt{rng} (pronounced as ``\textit{rung}'') is an algebraic structure $\alg{R; 0, -, +, \cdot}$ consisting of:
	\begin{dfnitems}
		\item a set $R$, called the \dfntxt{underlying set};
		\item a distinguished element $0 \in R$, called the \dfntxt{zero element};
		\item a unary operation $-: R \to R$, written as $x \mapsto -x$, called \dfntxt{negation};
		\item a binary operation $+: R \times R \to R$, written as $(x, y) \mapsto x + y$, called \dfntxt{addition};
		\item a binary operation $\cdot: R \times R \to R$, written as $(x, y) \mapsto x \cdot y$, called \dfntxt{multiplication};
	\end{dfnitems}
	satisfying the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Additive structure}: $\alg{R; 0, -, +}$ is an abelian group.
		\item \dfntxt{Associativity}: $(x \cdot y) \cdot z = x \cdot (y \cdot z)$ for all $x, y, z \in R$.
		\item \dfntxt{Left distributivity}: $x \cdot (y + z) = (x \cdot y) + (x \cdot z)$ for all $x, y, z \in R$.
		\item \dfntxt{Right distributivity}: $(x + y) \cdot z = (x \cdot z) + (y \cdot z)$ for all $x, y, z \in R$.
	\end{dfnitems}
\end{dfnbox}

The key ingredient present in the definition of a rng is \dfntxt{distributivity}, which establishes a link between two different binary operations. We begin our study of rngs by proving a simple (but important) result to demonstrate the utility of the distributive property.

\begin{thmbox}{Multiplying by Zero Yields Zero}
	\textbf{Theorem:} Let $\alg{R; 0, -, +, \cdot}$ be a rng. For any $x \in R$, we have $0 \cdot x = x \cdot 0 = 0$.
\tcblower
	\textit{Proof:} Let $x \in R$ be given. Because $0$ is the identity element of the abelian group $\langle R; 0, -, + \rangle$, we have $0 = 0 + 0$. Using left distributivity, it follows that $0 \cdot x = (0 + 0) \cdot x = (0 \cdot x) + (0 \cdot x)$, and by canceling one copy of $0 \cdot x$ on both sides, we conclude that $0 \cdot x = 0$. We similarly apply right distributivity to the expression $x \cdot 0 = x \cdot (0 + 0) = (x \cdot 0) + (x \cdot 0)$ to conclude that $x \cdot 0 = 0$.
\end{thmbox}

\begin{dfnbox}{Subrng, $S \le R$}
	Let $\alg{R; 0, -, +, \cdot}$ be a rng. A \dfntxt{subrng} of $\alg{R; 0, -, +, \cdot}$ is a subgroup $S \le \alg{R; 0, -, +}$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Closed under products}: If $x, y \in S$, then $x \cdot y$ in $S$.
	\end{dfnitems}
	We write $S \le R$ to indicate that $S$ is a subrng of $R$.
\end{dfnbox}

\begin{dfnbox}{Zero Rng, Trivial Rng, Nonzero Rng, Nontrivial Rng}
	The \dfntxt{zero rng} or \dfntxt{trivial rng} is the rng $\alg{ \{0\}; 0, -, +, \cdot }$ whose underlying set is a singleton containing the distinguished element $0$, and whose operations are defined by $-0 = 0 + 0 = 0 \cdot 0 = 0$. A rng is \dfntxt{nonzero} or \dfntxt{nontrivial} if its underlying set contains more than one element.
\end{dfnbox}

The zero rng is a subrng of every rng.

\begin{dfnbox}{Rng Homomorphism}
	Let $\alg{R; 0_R, -_R +_R, \cdot_R}$ and $\alg{S; 0_S, -_S +_S, \cdot_S}$ be rngs. A \dfntxt{rng homomorphism} is a function $f: R \to S$ that satisfies the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Preserves additive structure}: $f$ is a group homomorphism between the abelian groups $\alg{R; 0_R, -_R +_R}$ and $\alg{S; 0_S, -_S +_S}$.
		\item \dfntxt{Preserves products}: $f(x \cdot_R y) = f(x) \cdot_S f(y)$ for all $x, y \in R$.
	\end{dfnitems}
\end{dfnbox}

Every rng homomorphism is also a group homomorphism, so the theory and terminology of group homomorphisms is immediately applicable to rng homomorphisms. For example, the kernel of a rng homomorphism $f: R \to S$ is still defined to be the set
\[ \ker f \coloneq \{ x \in R : f(x) = 0_S \}. \]
As with groups, we will often refer to a rng $\alg{R; 0, -, +, \cdot}$ by the name of its underlying set $R$ and denote the multiplication operation $\cdot$ by juxtaposition. We will never denote addition, subtraction, or negation by juxtaposition, so the symbols $+$ and $-$ will always be used.

\begin{dfnbox}{Ideal, Left Ideal, Right Ideal, One-Sided Ideal, Two-Sided Ideal, $I \normalin R$}
	Let $R$ be a rng.
	\begin{dfnitems}
		\item A \dfntxt{left ideal} of $R$ is a subrng $I \le R$ that satisfies the following additional requirement:
		\begin{dfnitems}
			\item \dfntxt{Absorbs left multiplication}: $rx \in I$ for all $r \in R$ and $x \in I$.
		\end{dfnitems}
		\item A \dfntxt{right ideal} of $R$ is a subrng $I \le R$ that satisfies the following additional requirement:
		\begin{dfnitems}
			\item \dfntxt{Absorbs right multiplication}: $xr \in I$ for all $x \in I$ and $r \in R$.
		\end{dfnitems}
		\item A \dfntxt{one-sided ideal} of $R$ is a subrng $I \le R$ that is a left ideal or a right ideal (possibly both).
		\item A \dfntxt{two-sided ideal} of $R$, or simply an \dfntxt{ideal} of $R$, is a subrng $I \le R$ that is both a left ideal and a right ideal.
	\end{dfnitems}
	We write $I \normalin R$ to denote that $I$ is a (two-sided) ideal of $R$.
\end{dfnbox}

Every two-sided ideal is also a one-sided ideal, but the converse is not true.

\begin{thmbox}{Kernels are Ideals}
	\textbf{Theorem:} If $f: R \to S$ is a rng homomorphism, then $\ker f \normalin R$.
\tcblower
	\textit{Proof:} By definition, a rng homomorphism $f: R \to S$ is also a group homomorphism, so we already know that $\ker f$ is a subgroup of $R$. To prove that $\ker f$ is an ideal, we must show that it absorbs multiplication. Let $r \in R$ and $x \in \ker f$ be given. It follows that
	\[ f(rx) = f(r)f(x) = f(r)0 = 0
	\qquad \text{and} \qquad
	f(xr) = f(x)f(r) = 0f(r) = 0, \]
	so we conclude that $rx \in \ker f$ and $xr \in \ker f$.
\end{thmbox}

\begin{dfnbox}{Zero Divisor, Left Zero Divisor, Right Zero Divisor, Two-Sided Zero Divisor}
	Let $R$ be a rng.
	\begin{dfnitems}
		\item A \dfntxt{left zero divisor} is an element $x \in R$ for which there exists a nonzero element $y \in R \setminus \{0\}$ such that $xy = 0$.
		\item A \dfntxt{right zero divisor} is an element $x \in R$ for which there exists a nonzero element $y \in R \setminus \{0\}$ such that $yx = 0$.
		\item A \dfntxt{zero divisor} is an element $x \in R$ that is a left zero divisor or a right zero divisor.
		\item A \dfntxt{two-sided zero divisor} is an element $x \in R$ that is both a left zero divisor and a right zero divisor.
	\end{dfnitems}
\end{dfnbox}

Note that the terms ``ideal'' and ``zero divisor'' have opposite usage conventions. Unless otherwise specified, the lone term ``ideal'' means ``two-sided ideal,'' whereas the lone term ``zero divisor'' does \textit{not} mean ``two-sided zero divisor.''

\begin{dfnbox}{Ring}
	A \dfntxt{ring} is an algebraic structure $\alg{R; 0, 1, -, +, \cdot}$ consisting of a rng $\alg{R; 0, -, +, \cdot}$ and a distinguished element $1 \in R$, called the \dfntxt{identity element}, satisfying the following requirement:
	\begin{dfnitems}
		\item \dfntxt{Identity}: $1 \cdot x = x \cdot 1 = x$ for all $x \in R$.
	\end{dfnitems}
\end{dfnbox}

Note that the distinguished elements $0$ and $1$ in the definition of a ring need not be distinct. There is one ring in which $0 = 1$ holds.

\begin{thmbox}{$0 \ne 1$ in any Nontrivial Ring}
	\textbf{Theorem:} Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. If $0 = 1$, then $R = \{0\}$.
\tcblower
	\textit{Proof:} For any $x \in R$, we have $x = 1 \cdot x = 0 \cdot x = 0$.
\end{thmbox}

\begin{dfnbox}{Subring}
	Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. A \dfntxt{subring} of $\alg{R; 0, 1, -, +, \cdot}$ is a subrng $S \le \alg{R; 0, -, +, \cdot}$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Contains the identity}: $1 \in S$.
	\end{dfnitems}
\end{dfnbox}

In these notes, we will only use the notation $S \le R$ for \textit{subrngs}, not \textit{subrings}. This ensures consistency with the notation $I \normalin R$ for ideals (i.e., $I \normalin R \implies I \le R$), since an ideal is always a subrng, but not necessarily a subring.

The distinction between subrngs and subrings is subtle and can easily lead to confusion if these terms are not used carefully. For example, if $R$ is a ring and $S \le R$ is a subrng, it is possible for $S$ to be a ring in its own right without being a \textit{subring} of $R$. Consider $\Z \times \{0\} \le \Z \times \Z$; both of these rngs are rings, with identity elements $(1, 0) \in \Z \times \{0\}$ and $(1, 1) \in \Z \times \Z$. However, $\Z \times \{0\}$ is \textit{not} a subring of $\Z \times \Z$ because $(1, 1) \notin \Z \times \{0\}$. A subring \textit{must} contain the identity element of the original ring.

\begin{dfnbox}{Inverse, Left Inverse, Right Inverse, Two-Sided Inverse, Invertible, Unit}
	Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring, and let $x \in R$.
	\begin{dfnitems}
		\item A \dfntxt{left inverse} of $x$ is an element $y \in R$ such that $y \cdot x = 1$. If such an element exists, then we say that $x$ is \dfntxt{left-invertible}.
		\item A \dfntxt{right inverse} of $x$ is an element $y \in R$ such that $x \cdot y = 1$. If such an element exists, then we say that $x$ is \dfntxt{right-invertible}.
		\item A \dfntxt{two-sided inverse} of $x$, or simply an \dfntxt{inverse} of $x$, is an element $y \in R$ such that $y \cdot x = x \cdot y = 1$. If such an element exists, then we say that $x$ is \dfntxt{invertible}, and we call $x$ a \dfntxt{unit}.
	\end{dfnitems}
\end{dfnbox}

In ring theory, the word ``inverse'' used without further elaboration usually means ``two-sided inverse.''

\begin{thmbox}{Left and Right Invertibility Imply Two-Sided Invertibility}
	\textbf{Theorem:} Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. If $x \in R$ has both a left inverse $y \in R$ and a right inverse $z \in R$, then $y = z$, and $x$ is invertible.
\tcblower
	\textit{Proof:} Using the associativity of multiplication, observe that
	\[ y = y \cdot 1 = y \cdot (x \cdot z) = (y \cdot x) \cdot z = 1 \cdot z = z. \]
	Hence, $y = z$ is a two-sided inverse of $x$.
\end{thmbox}

\begin{thmbox}{Inverses are Unique and Invertible}
	\textbf{Corollary:} Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. If an element $x \in R$ is invertible, then it has a unique inverse. Moreover, that inverse is itself invertible, and $x$ is its unique inverse.
\tcblower
	\textit{Proof:} Let $y,z \in R$ be (two-sided) inverses of $x$. In particular, $y$ is a left inverse of $x$, and $z$ is a right inverse of $x$, so we can apply the preceding result to conclude that $y = z$.

	Observe that the relation $x \cdot y = y \cdot x = 1$ that defines inverses is symmetric in $x$ and $y$. Hence, if $y$ is an inverse of $x$, then $x$ is also an inverse of $y$.
\end{thmbox}

This result allows us to speak unambiguously of \textit{the} inverse of an invertible element of a ring.

\begin{dfnbox}{$R^\times$, $x^{-1}$}
	Let $\alg{R; 0, 1, -, +, \cdot}$ be a ring. The set of all units in $R$ is denoted by $R^\times$. For each $x \in R^\times$, we denote by $x^{-1}$ the unique inverse of $x$. Thus, we regard the map $x \mapsto x^{-1}$ as a unary operation ${}^{-1}: R^\times \to R^\times$.
\end{dfnbox}

Using this notation, we can restate the preceding result as $x = (x^{-1})^{-1}$ for all $x \in R^\times$.

\begin{thmbox}{$R^\times$ is a Group}
	\textbf{Theorem:} If $\alg{R; 0, 1, -, +, \cdot}$ is a ring, then $\alg{R^\times; 1, {}^{-1}, \cdot}$ is a group.
\end{thmbox}

\begin{thmbox}{An Ideal that Contains a Unit Contains Everything}
	\textbf{Theorem:} Let $R$ be a ring. If $I \le R$ is a one-sided ideal that contains a unit, then $I = R$.
\end{thmbox}

\begin{dfnbox}{Commutative Ring}
	A \dfntxt{commutative ring} is a ring $R$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Commutativity}: $xy = yx$ for all $x, y \in R$.
	\end{dfnitems}
\end{dfnbox}

\begin{dfnbox}{Domain}
	A \dfntxt{domain} is a nontrivial ring in which there are no zero divisors except $0$ itself.
\end{dfnbox}

\begin{dfnbox}{Integral Domain}
	An \dfntxt{integral domain} is a nontrivial commutative ring in which there are no zero divisors except $0$ itself.
\end{dfnbox}

\begin{dfnbox}{Principal Ideal, Generator}
	Let $R$ be a commutative ring, and let $r \in R$. The \dfntxt{principal ideal} generated by $r$, denoted by $\gen{r}$, is the subset of $R$ consisting of all multiples of $r$.
	\[ \gen{r} \coloneq \{ rx : x \in R \} \]
	An ideal $I \normalin R$ is \dfntxt{principal} if there exists an element $r \in I$ such that $R = \gen{r}$. In this case, we say that $r$ \dfntxt{generates} the ideal $I$, and we call $r$ the \dfntxt{generator} of $I$.
\end{dfnbox}

\begin{dfnbox}{Principal Ideal Domain, PID}
	A \dfntxt{principal ideal domain} is an integral domain in which every ideal is principal.
\end{dfnbox}

\begin{dfnbox}{Divides, Divisibility Relation}
	Let $R$ be a commutative rng. An element $a \in R$ \dfntxt{divides} an element $b \in R$ if there exists an element $q \in R$ such that $b = qa$. We write $a \mid b$ to denote that $a$ divides $b$, and we call this relation ${\mid} \subseteq R \times R$ the \dfntxt{divisibility relation} on $R$.
\end{dfnbox}

The divisibility relation $\mid$ is always reflexive in a ring, but can fail to be reflexive in a rng. Note that $0$ does not divide any element of a rng except $0$, but every element of a rng divides $0$.

\begin{thmbox}{Divisibility is Transitive}
	\textbf{Theorem:} The divisibility relation on any rng is transitive.
\tcblower
	\textit{Proof:} Let $R$ be a rng, and let $a, b, c \in R$. If $a \mid b$ and $b \mid c$, then there exist $x, y \in R$ such that $b = xa$ and $c = yb$. It follows that $c = yxa$, which proves that $a \mid c$.
\end{thmbox}

\begin{dfnbox}{Euclidean Valuation}
	A \dfntxt{Euclidean valuation} on a commutative ring $R$ is a function $\nu: R \setminus \{0\} \to W$ into a well-ordered set $(W, \le)$ that has the following property: for all $a, b \in R$, if $b \ne 0$, then either $b \mid a$, or there exist $q, r \in R$ with $r \ne 0$ and $\nu(r) < \nu(b)$ such that $a = qb + r$.
\end{dfnbox}

\begin{dfnbox}{Euclidean Domain, ED}
	A \dfntxt{Euclidean domain}, or \dfntxt{ED}, is an integral domain on which there exists a Euclidean valuation.
\end{dfnbox}

This complicated definition formalizes a simple idea: a Euclidean domain is a set in which it is possible to divide two elements with remainder. In any commutative ring, it is possible to define a completely unhelpful division-with-remainder operation by declaring $a$ divided by $b$ to have quotient $0$ and remainder $a$. In order for this operation to make useful progress, the remainder $r$ has to be, in some sense, ``smaller'' than the divisor $b$.

A Euclidean valuation formalizes this notion by requiring that $\nu(r) < \nu(b)$. The actual values taken by the function $\nu$ are completely unimportant, as long as they exist in a set that does not contain an infinite descending chain $\nu(x_1) > \nu(x_2) > \cdots$. This guarantees that repeated division with remainder (as performed in the Euclidean algorithm) always terminates in a finite number of steps.

\begin{thmbox}{Every ED is a PID}
	\textbf{Theorem:} Every Euclidean domain is a principal ideal domain.
\tcblower
	\textit{Proof:} Let $R$ be a Euclidean domain, and let an ideal $I \normalin R$ be given. If $I = \{0\}$, then $I$ is the principal ideal generated by $0$. Otherwise, let $\nu: R \setminus \{0\} \to W$ be a Euclidean valuation on $R$ for some well-ordered set $W$. Choose an element $b \in I \setminus \{0\}$ that minimizes $\nu(b)$. (The well-ordering of $W$ guarantees that such an element exists.)

	We claim that $I = \gen{b}$. It is clear that $\gen{b} \subseteq I$, since an ideal $I$ must contain all multiples of an element $b \in I$. To see that $I \subseteq \gen{b}$, take an arbitrary element $a \in I$. Either $b \mid a$, in which case $a \in \gen{b}$, or there exist $q, r \in R$ with $r \ne 0$ and $\nu(r) < \nu(b)$ such that $a = qb + r$. Since $a \in I$ and $b \in I$, it follows that $qb \in I$, and hence that $a - qb = r \in I$. This is impossible, as the condition $\nu(r) < \nu(b)$ would contradict the minimality of $\nu(b)$. Hence, it must be the case that $a \in \gen{b}$.
\end{thmbox}



\chapter{Field Theory}

\begin{dfnbox}{Field}
	A \dfntxt{field} is a commutative ring $\alg{K; 0, 1, -, +, \cdot}$ that satisfies the following additional requirements:
	\begin{dfnitems}
		\item \dfntxt{Nontriviality}: $0 \ne 1$.
		\item \dfntxt{Invertibility}: Every element of $K \setminus \{0\}$ has a (two-sided) inverse, i.e., $K^\times = K \setminus \{0\}$.
	\end{dfnitems}
\end{dfnbox}

\begin{dfnbox}{Field Homomorphism}
	A \dfntxt{field homomorphism} is a ring homomorphism $f: K \to L$ where the domain $K$ and codomain $L$ are both fields.
\end{dfnbox}

Note that field homomorphisms are defined as \textit{ring} homomorphisms, not \textit{rng} homomorphisms, so they are required to map $1$ to $1$.

We now prove a crucial fact that markedly distinguishes field theory from group theory and ring theory.

\begin{thmbox}{Fields Have Two Ideals}
	\textbf{Theorem:} Let $K$ be a field. If $I \le K$ is a one-sided ideal, then either $I = \{0\}$ or $I = K$.
\tcblower
	\textit{Proof:} A one-sided ideal $I \le K$ must contain $0$ by definition. If $I$ contains any other element of $K$, then $I$ contains a unit, and hence contains every element of $K$.
\end{thmbox}

\begin{thmbox}{Every Field Homomorphism is a Monomorphism}
	\textbf{Theorem:} Every rng homomorphism $f: K \to R$ from a field $K$ to a rng $R$ is either injective or trivial (i.e., $f(x) = 0$ for all $x \in K$).
\tcblower
	\textit{Proof:} Either $\ker f = \{0\}$, in which case $f$ is injective, or $\ker f = K$, in which case $f$ is trivial.
\end{thmbox}

In field theory, it is conventional to regard a monomorphism $f: K \injto L$ as an \textit{embedding} of $K$ into $L$. Under this interpretation, the preceding result shows that the only possible relationship between two fields (via a field homomorphism) is for one to be contained inside the other. For this reason, field theory does not adopt the language of homomorphisms that permeates group theory and ring theory. Instead, field theory is written in the language of \textit{subfields} and \textit{field extensions}.

\begin{dfnbox}{Subfield}
	Let $K$ be a field. A \dfntxt{subfield} of $K$ is a subring $L \le K$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Closed under inversion}: If $x \in L \setminus \{0\}$, then $x^{-1} \in L$.
	\end{dfnitems}
\end{dfnbox}

Not every subring of a field is a subfield. For example, $\Z$ is a subring but not a subfield of $\Q$.

\begin{dfnbox}{Field Extension, $L/K$}
	Let $L$ be a field. If $K$ is a subfield of $L$, then we say that $L$ is an \dfntxt{extension} of $K$, and we say that $L/K$ (pronounced as ``$L$ \textit{over} $K$'') is a \dfntxt{field extension}.
\end{dfnbox}

Somewhat confusingly, the notation $L/K$ is not intended to represent a quotient of any kind. It is simply a strange (but historically traditional) notation for an ordered pair $(L, K)$ of fields, carrying the additional information that the second field is contained in the first. Field theory has no use for quotients, since fields have no nontrivial proper ideals.

\begin{dfnbox}{Degree, $[L : K]$, Finite Extension}
	The \dfntxt{degree} of a field extension $L/K$, denoted by $[L : K]$, is the dimension of $L$ as a vector space over $K$. If $[L : K]$ is finite, then we call $L/K$ a \dfntxt{finite extension}.
\end{dfnbox}

Note that the individual fields $K$ and $L$ involved in a finite extension $L/K$ are allowed to have infinite cardinality. The phrase ``finite extension'' specifies that the \textit{extension} is finite, not that the \textit{fields} are finite.

\begin{dfnbox}{Algebraic Element, Transcendental Element}
	Let $L/K$ be a field extension. We say that an element $\alpha \in L$ is \dfntxt{algebraic} over $K$ if there exists a polynomial $p \in K[x]$ such that $p(\alpha) = 0$. If no such polynomial exists, then we say that $\alpha$ is \dfntxt{transcendental} over $K$.
\end{dfnbox}

\begin{dfnbox}{Algebraic Extension, Transcendental Extension}
	An \dfntxt{algebraic extension} is a field extension $L/K$ in which every element of $L$ is algebraic over $K$. On the other hand, a \dfntxt{transcendental extension} is a field extension $L/K$ in which $L$ contains an element that is transcendental over $K$.
\end{dfnbox}

\begin{thmbox}{Minimal Polynomials Exist}
	\textbf{Theorem:} Let $L/K$ be a field extension. If $\alpha \in L$ is algebraic over $K$, then there exists a unique monic polynomial in $K[x]$ of minimal degree satisfying $p(\alpha) = 0$.
\tcblower
	\textit{Proof:} The existence of such a polynomial is clear, since by the hypothesis that $\alpha$ is algebraic over $K$, there exists a polynomial $p \in K[x]$ satisfying $p(\alpha) = 0$. (This polynomial can be made monic by dividing it by its leading coefficient.) Because the degrees of monic polynomials (i.e., natural numbers) are well-ordered, we can conclude that there exists a minimal-degree monic polynomial having this property.

	We verify uniqueness by contradiction. Suppose that there exist two distinct monic polynomials $p, q \in K[x]$ of minimal degree that satisfy $p(\alpha) = q(\alpha) = 0$. It follows that $p-q \in K[x]$ is a polynomial of strictly smaller degree that satisfies $(p-q)(\alpha) = p(\alpha) - q(\alpha) = 0 - 0 = 0$, contradicting the minimality of $p$ and $q$.
\end{thmbox}

\begin{dfnbox}{Minimal Polynomial}
	Let $L/K$ be a field extension, and let $\alpha \in L$ be algebraic over $K$. The \dfntxt{minimal polynomial} of $\alpha$ over $K$ is the unique monic polynomial $p \in K[x]$ of minimal degree satisfying $p(\alpha) = 0$.
\end{dfnbox}

\begin{thmbox}{Minimal Polynomials are Irreducible}
	\textbf{Theorem:} Let $L/K$ be a field extension. If $p \in K[x]$ is the minimal polynomial of an algebraic element $\alpha \in L$, then $p$ is irreducible in $K[x]$.
\tcblower
	\textit{Proof:} If $p$ could be written as the product of two non-constant polynomials $q, r \in K[x]$, then $0 = p(\alpha) = q(\alpha) r(\alpha)$ would imply that $q(\alpha) = 0$ or $r(\alpha) = 0$, contradicting the minimality of $p$.
\end{thmbox}

\begin{dfnbox}{Annihilator Ideal}
	Let $L/K$ be a field extension. The \dfntxt{annihilator ideal} of an element $\alpha \in L$ over $K$ is the subset of $K[x]$
\end{dfnbox}

\begin{thmbox}{Minimal Polynomial Divides Every Other Polynomial}
\end{thmbox}

\begin{dfnbox}{Finite Field}
	A \dfntxt{finite field} is a field whose underlying set has finite cardinality.
\end{dfnbox}

\begin{thmbox}{Finite Fields have Cyclic Multiplicative Groups}
	\textbf{Theorem:} If $K$ is a finite field, then $K^\times$ is a cyclic group.
\end{thmbox}

\begin{dfnbox}{Simple Extension, Primitive Element}
	A field extension $L/K$ is called a \dfntxt{simple extension} if there exists an element $\alpha \in L$ such that $L = K(\alpha)$, i.e., every element of $L$ can be expressed as a rational function of $\alpha$ with coefficients in $K$. Such an element $\alpha$ is called a \dfntxt{primitive element} of $L$ over $K$.
\end{dfnbox}

\begin{thmbox}{Primitive Element Theorem}
	\textbf{Theorem:} Let $K$ be a field. A finite extension $L/K$ is simple if and only if there exist finitely many intermediate subfields $F$ satisfying $K \le F \le L$.
\tcblower
	\textit{Proof:} We first consider the case of a finite base field $K$. In this case, the extension field $L$, being a finite extension of a finite field, is also finite. This implies that both sides of the desired ``if and only if'' statement are true. In particular, $L$ has finitely many subfields, and $L^\times$ is a cyclic group, so its generator is a primitive element.

	Now suppose that the base field $K$ is infinite. There are two implications that need to be proven.
	\begin{dfnitems}
		\item Suppose $L/K$ is a simple extension with primitive element $\alpha \in L$,
		\item Suppose there are finitely many intermediate subfields $F$ satisfying $K \le F \le L$.
	\end{dfnitems}
\end{thmbox}

\begin{dfnbox}{(Algebraic) Number Field}
	An \dfntxt{algebraic number field}, or simply a \dfntxt{number field}, is a finite extension of $\Q$.
\end{dfnbox}

\begin{dfnbox}{Ring of Integers}
	Let $K$ be an algebraic number field. An element $\alpha \in K$ is an \dfntxt{algebraic integer} if there exists a monic polynomial $p \in \Z[x]$ such that $p(\alpha) = 0$. The set of all algebraic integers in $K$ is called the \dfntxt{ring of integers} of $K$, denoted by $\symcal{O}_K$.
\end{dfnbox}



\chapter{Differential Algebra}

\begin{dfnbox}{Derivation}
	A \dfntxt{derivation} on a rng $\alg{R; 0, -, +, \cdot}$ is a unary operation $\dd: R \to R$ that satisfies the following requirements:
	\begin{dfnitems}
		\item \dfntxt{Additivity}: $\dd(x + y) = \dd(x) + \dd(y)$ for all $x, y \in R$.
		\item \dfntxt{Leibniz rule}: $\dd(x \cdot y) = \dd(x) \cdot y + x \cdot \dd(y)$ for all $x, y \in R$.
	\end{dfnitems}
\end{dfnbox}

\begin{dfnbox}{Constant}
	Let $R$ be a rng, and $\dd: R \to R$ be a derivation on $R$. An element $x \in R$ is \dfntxt{constant} with respect to $\dd$ if $\dd(x) = 0$.
\end{dfnbox}

\begin{dfnbox}{Differential Rng}
	A \dfntxt{differential rng} is an algebraic structure $\alg{R; 0, -, \dd, +, \cdot}$ consisting of a rng $\alg{R; 0, -, +, \cdot}$ and a derivation $\dd: R \to R$ on $\alg{R; 0, -, +, \cdot}$.
\end{dfnbox}

\begin{dfnbox}{Subrng of Constants, $\const(R)$}
	Let $R$ be a differential rng. The \dfntxt{subrng of constants} of $R$, denoted by $\const(R)$, is the set of constants in $R$.
	\[ \const(R) \coloneq \{ x \in R : \dd(x) = 0 \} \]
\end{dfnbox}

\begin{thmbox}{Constants Form a Subrng}
	\textbf{Theorem:} If $R$ is a differential rng, then $\const(R) \le R$.
\tcblower
	\textit{Proof:} It suffices to show that $\const(R)$ is closed under sums and products. Let $x, y \in \const(R)$.
	\[ \dd(x + y) = \dd(x) + \dd(y) = 0 + 0 = 0 \implies x + y \in \const(R) \]
	\[ \dd(x \cdot y) = \dd(x) \cdot y + x \cdot \dd(y) = 0 \cdot y + x \cdot 0 = 0 \implies x \cdot y \in \const(R) \]
\end{thmbox}

\begin{dfnbox}{Antiderivative, Integrable}
	Let $R$ be a differential rng, and let $f \in R$. An \dfntxt{antiderivative} of $f$ is an element $F \in R$ such that $\dd(F) = f$. If $f$ has an antiderivative, then we say that $f$ is \dfntxt{integrable}.
\end{dfnbox}

\begin{dfnbox}{Differential Subrng, Differential Rng Extension}
	Let $R$ be a differential rng. A \dfntxt{differential subrng} of $R$ is a subrng $S \le R$ that satisfies the following additional requirement:
	\begin{dfnitems}
		\item \dfntxt{Closed under derivation:} If $x \in S$, then $\dd(x) \in S$.
	\end{dfnitems}
	If $S$ is a differential subrng of $R$, then we say that $R$ is a \dfntxt{differential extension} of $S$, and say that $R/S$ (pronounced as ``$R$ \textit{over} $S$'') is a \dfntxt{differential rng extension}.
\end{dfnbox}

Note that in a differential rng extension $R/S$, the derivation on $R$ is required to coincide with the derivation on $S$ when restricted to $S$. This distinguishes a differential rng extension $R/S$ from a rng extension $R/S$ where $R$ happens to be a differential rng.

\begin{dfnbox}{Differential Ring}
	A \dfntxt{differential ring} is an algebraic structure $\alg{R; 0, 1, -, \dd, +, \cdot}$ consisting of a ring $\alg{R; 0, 1, -, +, \cdot}$ and a derivation $\dd: R \to R$ on $\alg{R; 0, -, +, \cdot}$.
\end{dfnbox}

The terms \dfntxt{differential integral domain}, \dfntxt{differential PID}, \dfntxt{differential field}, etc.\ are defined analogously.

\begin{thmbox}{$1$ is a Constant}
	\textbf{Theorem:} In any differential ring, $\dd(1) = 0$.
\tcblower
	\textit{Proof:} We apply the Leibniz rule to $1 = 1 \cdot 1$.
	\[ \dd(1) = \dd(1 \cdot 1) = \dd(1) \cdot 1 + 1 \cdot \dd(1) = \dd(1) + \dd(1) \]
	By cancellation, this implies that $\dd(1) = 0$.
\end{thmbox}

This result implies that the \textit{subrng} of constants of a differential ring is, in fact, a \textit{subring}. Hence, when $R$ is a differential ring, we will refer to $\const(R)$ as its \dfntxt{subring of constants}.

\begin{thmbox}{Algebraic Extensions Don't Create Antiderivatives}
	\textbf{Theorem:} Let $K$ be a differential field of characteristic zero. If $f \in K$ does not have an antiderivative in $K$, then $f$ cannot have an antiderivative in any algebraic differential extension of $K$.
\tcblower
	\textit{Proof:} We proceed by contraposition. Suppose that there exists an algebraic differential extension $L/K$ having an element $g \in L$ for which $f = \dd(g)$. Let $\Tr: K[g] \to K$ denote the trace map, and recall that $\Tr$ commutes with $\dd$. Since $f \in K$, we have $\Tr(f) = nf$, where $n \coloneq [K[g] : K]$. It follows that
	\[ \dd \left( \frac{\Tr(g)}{n} \right) = \frac{1}{n} \dd(\Tr(g)) = \frac{1}{n} \Tr(\dd(g)) = \frac{1}{n} \Tr(f) = f \]
	which shows that $\Tr(g)/n \in K$ is an antiderivative of $f$.
\end{thmbox}



\end{document}
